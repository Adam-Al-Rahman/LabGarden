{
  
    
        "post0": {
            "title": "Getting started with Tensorflow",
            "content": "00. Getting started with TensorFlow: A guide to the fundamentals . What is TensorFlow? . TensorFlow is an open-source end-to-end machine learning library for preprocessing data, modelling data and serving models (getting them into the hands of others). . Why use TensorFlow? . Rather than building machine learning and deep learning models from scratch, it&#39;s more likely you&#39;ll use a library such as TensorFlow. This is because it contains many of the most common machine learning functions you&#39;ll want to use. . What we&#39;re going to cover . TensorFlow is vast. But the main premise is simple: turn data into numbers (tensors) and build machine learning algorithms to find patterns in them. . In this notebook we cover some of the most fundamental TensorFlow operations, more specificially: . Introduction to tensors (creating tensors) | Getting information from tensors (tensor attributes) | Manipulating tensors (tensor operations) | Tensors and NumPy | Using @tf.function (a way to speed up your regular Python functions) | Using GPUs with TensorFlow | Exercises to try | . Things to note: . Many of the conventions here will happen automatically behind the scenes (when you build a model) but it&#39;s worth knowing so if you see any of these things, you know what&#39;s happening. | For any TensorFlow function you see, it&#39;s important to be able to check it out in the documentation, for example, going to the Python API docs for all functions and searching for what you need: https://www.tensorflow.org/api_docs/python/ (don&#39;t worry if this seems overwhelming at first, with enough practice, you&#39;ll get used to navigating the documentaiton). | . import tensorflow as tf print(tf.__version__) # find the version number (should be 2.x+) . 2.4.1 . Creating Tensors with tf.constant() . As mentioned before, in general, you usually won&#39;t create tensors yourself. This is because TensorFlow has modules built-in (such as tf.io and tf.data) which are able to read your data sources and automatically convert them to tensors and then later on, neural network models will process these for us. . But for now, because we&#39;re getting familar with tensors themselves and how to manipulate them, we&#39;ll see how we can create them ourselves. . We&#39;ll begin by using tf.constant(). . scalar = tf.constant(7) scalar . &lt;tf.Tensor: shape=(), dtype=int32, numpy=7&gt; . A scalar is known as a rank 0 tensor. Because it has no dimensions (it&#39;s just a number). . 🔑 Note: For now, you don&#39;t need to know too much about the different ranks of tensors (but we will see more on this later). The important point is knowing tensors can have an unlimited range of dimensions (the exact amount will depend on what data you&#39;re representing). . scalar.ndim . 0 . vector = tf.constant([10, 10]) vector . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([10, 10], dtype=int32)&gt; . vector.ndim . 1 . matrix = tf.constant([[10, 7], [7, 10]]) matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[10, 7], [ 7, 10]], dtype=int32)&gt; . matrix.ndim . 2 . By default, TensorFlow creates tensors with either an int32 or float32 datatype. . This is known as 32-bit precision (the higher the number, the more precise the number, the more space it takes up on your computer). . another_matrix = tf.constant([[10., 7.], [3., 2.], [8., 9.]], dtype=tf.float16) # specify the datatype with &#39;dtype&#39; another_matrix . &lt;tf.Tensor: shape=(3, 2), dtype=float16, numpy= array([[10., 7.], [ 3., 2.], [ 8., 9.]], dtype=float16)&gt; . another_matrix.ndim . 2 . tensor = tf.constant([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18]]]) tensor . &lt;tf.Tensor: shape=(3, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18]]], dtype=int32)&gt; . tensor.ndim . 3 . This is known as a rank 3 tensor (3-dimensions), however a tensor can have an arbitrary (unlimited) amount of dimensions. . For example, you might turn a series of images into tensors with shape (224, 224, 3, 32), where: . 224, 224 (the first 2 dimensions) are the height and width of the images in pixels. | 3 is the number of colour channels of the image (red, green blue). | 32 is the batch size (the number of images a neural network sees at any one time). | . All of the above variables we&#39;ve created are actually tensors. But you may also hear them referred to as their different names (the ones we gave them): . scalar: a single number. | vector: a number with direction (e.g. wind speed with direction). | matrix: a 2-dimensional array of numbers. | tensor: an n-dimensional arrary of numbers (where n can be any number, a 0-dimension tensor is a scalar, a 1-dimension tensor is a vector). | . To add to the confusion, the terms matrix and tensor are often used interchangably. . Going forward since we&#39;re using TensorFlow, everything we refer to and use will be tensors. . For more on the mathematical difference between scalars, vectors and matrices see the visual algebra post by Math is Fun. . . Creating Tensors with tf.Variable() . You can also (although you likely rarely will, because often, when working with data, tensors are created for you automatically) create tensors using tf.Variable(). . The difference between tf.Variable() and tf.constant() is tensors created with tf.constant() are immutable (can&#39;t be changed, can only be used to create a new tensor), where as, tensors created with tf.Variable() are mutable (can be changed). . changeable_tensor = tf.Variable([10, 7]) unchangeable_tensor = tf.constant([10, 7]) changeable_tensor, unchangeable_tensor . (&lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([10, 7], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([10, 7], dtype=int32)&gt;) . Now let&#39;s try to change one of the elements of the changable tensor. . changeable_tensor[0] = 7 changeable_tensor . TypeError Traceback (most recent call last) &lt;ipython-input-14-daecfbad2415&gt; in &lt;module&gt;() 1 # Will error (requires the .assign() method) -&gt; 2 changeable_tensor[0] = 7 3 changeable_tensor TypeError: &#39;ResourceVariable&#39; object does not support item assignment . To change an element of a tf.Variable() tensor requires the assign() method. . changeable_tensor[0].assign(7) changeable_tensor . &lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([7, 7], dtype=int32)&gt; . Now let&#39;s try to change a value in a tf.constant() tensor. . unchangeable_tensor[0].assign(7) unchangleable_tensor . AttributeError Traceback (most recent call last) &lt;ipython-input-16-3947b974feb9&gt; in &lt;module&gt;() 1 # Will error (can&#39;t change tf.constant()) -&gt; 2 unchangeable_tensor[0].assign(7) 3 unchangleable_tensor AttributeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object has no attribute &#39;assign&#39; . Which one should you use? tf.constant() or tf.Variable()? . It will depend on what your problem requires. However, most of the time, TensorFlow will automatically choose for you (when loading data or modelling data). . Creating random tensors . Random tensors are tensors of some abitrary size which contain random numbers. . Why would you want to create random tensors? . This is what neural networks use to intialize their weights (patterns) that they&#39;re trying to learn in the data. . For example, the process of a neural network learning often involves taking a random n-dimensional array of numbers and refining them until they represent some kind of pattern (a compressed way to represent the original data). . How a network learns A network learns by starting with random patterns (1) then going through demonstrative examples of data (2) whilst trying to update its random patterns to represent the examples (3). . We can create random tensors by using the tf.random.Generator class. . random_1 = tf.random.Generator.from_seed(42) # set the seed for reproducibility random_1 = random_1.normal(shape=(3, 2)) # create tensor from a normal distribution random_2 = tf.random.Generator.from_seed(42) random_2 = random_2.normal(shape=(3, 2)) # Are they equal? random_1, random_2, random_1 == random_2 . (&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy= array([[-0.7565803 , -0.06854702], [ 0.07595026, -1.2573844 ], [-0.23193765, -1.8107855 ]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy= array([[-0.7565803 , -0.06854702], [ 0.07595026, -1.2573844 ], [-0.23193765, -1.8107855 ]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy= array([[ True, True], [ True, True], [ True, True]])&gt;) . The random tensors we&#39;ve made are actually pseudorandom numbers (they appear as random, but really aren&#39;t). . If we set a seed we&#39;ll get the same random numbers (if you&#39;ve ever used NumPy, this is similar to np.random.seed(42)). . Setting the seed says, &quot;hey, create some random numbers, but flavour them with X&quot; (X is the seed). . What do you think will happen when we change the seed? . random_3 = tf.random.Generator.from_seed(42) random_3 = random_3.normal(shape=(3, 2)) random_4 = tf.random.Generator.from_seed(11) random_4 = random_4.normal(shape=(3, 2)) # Check the tensors and see if they are equal random_3, random_4, random_1 == random_3, random_3 == random_4 . (&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy= array([[-0.7565803 , -0.06854702], [ 0.07595026, -1.2573844 ], [-0.23193765, -1.8107855 ]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy= array([[ 0.2730574 , -0.29925638], [-0.3652325 , 0.61883307], [-1.0130816 , 0.2829171 ]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy= array([[ True, True], [ True, True], [ True, True]])&gt;, &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy= array([[False, False], [False, False], [False, False]])&gt;) . What if you wanted to shuffle the order of a tensor? . Wait, why would you want to do that? . Let&#39;s say you working with 15,000 images of cats and dogs and the first 10,000 images of were of cats and the next 5,000 were of dogs. This order could effect how a neural network learns (it may overfit by learning the order of the data), instead, it might be a good idea to move your data around. . not_shuffled = tf.constant([[10, 7], [3, 4], [2, 5]]) # Gets different results each time tf.random.shuffle(not_shuffled) . &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[10, 7], [ 2, 5], [ 3, 4]], dtype=int32)&gt; . tf.random.shuffle(not_shuffled, seed=42) . &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[ 2, 5], [ 3, 4], [10, 7]], dtype=int32)&gt; . Wait... why didn&#39;t the numbers come out the same? . It&#39;s due to rule #4 of the tf.random.set_seed() documentation. . &quot;4. If both the global and the operation seed are set:Both seeds are used in conjunction to determine the random sequence.&quot; tf.random.set_seed(42) sets the global seed, and the seed parameter in tf.random.shuffle(seed=42) sets the operation seed. . Because, &quot;Operations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds. This sets the global seed.&quot; . # Set the global random seed tf.random.set_seed(42) # Set the operation random seed tf.random.shuffle(not_shuffled, seed=42) . &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[10, 7], [ 3, 4], [ 2, 5]], dtype=int32)&gt; . tf.random.set_seed(42) # if you comment this out you&#39;ll get different results # Set the operation random seed tf.random.shuffle(not_shuffled) . &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[ 3, 4], [ 2, 5], [10, 7]], dtype=int32)&gt; . Other ways to make tensors . Though you might rarely use these (remember, many tensor operations are done behind the scenes for you), you can use tf.ones() to create a tensor of all ones and tf.zeros() to create a tensor of all zeros. . tf.ones(shape=(3, 2)) . &lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy= array([[1., 1.], [1., 1.], [1., 1.]], dtype=float32)&gt; . tf.zeros(shape=(3, 2)) . &lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy= array([[0., 0.], [0., 0.], [0., 0.]], dtype=float32)&gt; . You can also turn NumPy arrays in into tensors. . Remember, the main difference between tensors and NumPy arrays is that tensors can be run on GPUs. . 🔑 Note: A matrix or tensor is typically represented by a capital letter (e.g. X or A) where as a vector is typically represented by a lowercase letter (e.g. y or b). . import numpy as np numpy_A = np.arange(1, 25, dtype=np.int32) # create a NumPy array between 1 and 25 A = tf.constant(numpy_A, shape=[2, 4, 3]) # note: the shape total (2*4*3) has to match the number of elements in the array numpy_A, A . (array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], dtype=int32), &lt;tf.Tensor: shape=(2, 4, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], [[13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24]]], dtype=int32)&gt;) . Getting information from tensors (shape, rank, size) . There will be times when you&#39;ll want to get different pieces of information from your tensors, in particuluar, you should know the following tensor vocabulary: . Shape: The length (number of elements) of each of the dimensions of a tensor. | Rank: The number of tensor dimensions. A scalar has rank 0, a vector has rank 1, a matrix is rank 2, a tensor has rank n. | Axis or Dimension: A particular dimension of a tensor. | Size: The total number of items in the tensor. | . You&#39;ll use these especially when you&#39;re trying to line up the shapes of your data to the shapes of your model. For example, making sure the shape of your image tensors are the same shape as your models input layer. . We&#39;ve already seen one of these before using the ndim attribute. Let&#39;s see the rest. . rank_4_tensor = tf.zeros([2, 3, 4, 5]) rank_4_tensor . &lt;tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy= array([[[[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]], [[[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]]], dtype=float32)&gt; . rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor) . (TensorShape([2, 3, 4, 5]), 4, &lt;tf.Tensor: shape=(), dtype=int32, numpy=120&gt;) . print(&quot;Datatype of every element:&quot;, rank_4_tensor.dtype) print(&quot;Number of dimensions (rank):&quot;, rank_4_tensor.ndim) print(&quot;Shape of tensor:&quot;, rank_4_tensor.shape) print(&quot;Elements along axis 0 of tensor:&quot;, rank_4_tensor.shape[0]) print(&quot;Elements along last axis of tensor:&quot;, rank_4_tensor.shape[-1]) print(&quot;Total number of elements (2*3*4*5):&quot;, tf.size(rank_4_tensor).numpy()) # .numpy() converts to NumPy array . Datatype of every element: &lt;dtype: &#39;float32&#39;&gt; Number of dimensions (rank): 4 Shape of tensor: (2, 3, 4, 5) Elements along axis 0 of tensor: 2 Elements along last axis of tensor: 5 Total number of elements (2*3*4*5): 120 . You can also index tensors just like Python lists. . rank_4_tensor[:2, :2, :2, :2] . &lt;tf.Tensor: shape=(2, 2, 2, 2), dtype=float32, numpy= array([[[[0., 0.], [0., 0.]], [[0., 0.], [0., 0.]]], [[[0., 0.], [0., 0.]], [[0., 0.], [0., 0.]]]], dtype=float32)&gt; . rank_4_tensor[:1, :1, :1, :] . &lt;tf.Tensor: shape=(1, 1, 1, 5), dtype=float32, numpy=array([[[[0., 0., 0., 0., 0.]]]], dtype=float32)&gt; . rank_2_tensor = tf.constant([[10, 7], [3, 4]]) # Get the last item of each row rank_2_tensor[:, -1] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([7, 4], dtype=int32)&gt; . You can also add dimensions to your tensor whilst keeping the same information present using tf.newaxis. . rank_3_tensor = rank_2_tensor[..., tf.newaxis] # in Python &quot;...&quot; means &quot;all dimensions prior to&quot; rank_2_tensor, rank_3_tensor # shape (2, 2), shape (2, 2, 1) . (&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[10, 7], [ 3, 4]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy= array([[[10], [ 7]], [[ 3], [ 4]]], dtype=int32)&gt;) . You can achieve the same using tf.expand_dims(). . tf.expand_dims(rank_2_tensor, axis=-1) # &quot;-1&quot; means last axis . &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy= array([[[10], [ 7]], [[ 3], [ 4]]], dtype=int32)&gt; . Manipulating tensors (tensor operations) . Finding patterns in tensors (numberical representation of data) requires manipulating them. . Again, when building models in TensorFlow, much of this pattern discovery is done for you. . Basic operations . You can perform many of the basic mathematical operations directly on tensors using Pyhton operators such as, +, -, *. . tensor = tf.constant([[10, 7], [3, 4]]) tensor + 10 . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[20, 17], [13, 14]], dtype=int32)&gt; . Since we used tf.constant(), the original tensor is unchanged (the addition gets done on a copy). . tensor . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[10, 7], [ 3, 4]], dtype=int32)&gt; . Other operators also work. . tensor * 10 . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[100, 70], [ 30, 40]], dtype=int32)&gt; . tensor - 10 . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 0, -3], [-7, -6]], dtype=int32)&gt; . You can also use the equivalent TensorFlow function. Using the TensorFlow function (where possible) has the advantage of being sped up later down the line when running as part of a TensorFlow graph. . tf.multiply(tensor, 10) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[100, 70], [ 30, 40]], dtype=int32)&gt; . tensor . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[10, 7], [ 3, 4]], dtype=int32)&gt; . Matrix mutliplication . One of the most common operations in machine learning algorithms is matrix multiplication. . TensorFlow implements this matrix multiplication functionality in the tf.matmul() method. . The main two rules for matrix multiplication to remember are: . The inner dimensions must match: (3, 5) @ (3, 5) won&#39;t work | (5, 3) @ (3, 5) will work | (3, 5) @ (5, 3) will work | . | The resulting matrix has the shape of the inner dimensions: (5, 3) @ (3, 5) -&gt; (3, 3) | (3, 5) @ (5, 3) -&gt; (5, 5)🔑 Note: &#39;@&#39; in Python is the symbol for matrix multiplication. . | . | print(tensor) tf.matmul(tensor, tensor) . tf.Tensor( [[10 7] [ 3 4]], shape=(2, 2), dtype=int32) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[121, 98], [ 42, 37]], dtype=int32)&gt; . tensor @ tensor . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[121, 98], [ 42, 37]], dtype=int32)&gt; . Both of these examples work because our tensor variable is of shape (2, 2). . What if we created some tensors which had mismatched shapes? . X = tf.constant([[1, 2], [3, 4], [5, 6]]) # Create another (3, 2) tensor Y = tf.constant([[7, 8], [9, 10], [11, 12]]) X, Y . (&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[1, 2], [3, 4], [5, 6]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[ 7, 8], [ 9, 10], [11, 12]], dtype=int32)&gt;) . X @ Y . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-43-62e1e4702ffd&gt; in &lt;module&gt;() 1 # Try to matrix multiply them (will error) -&gt; 2 X @ Y /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y) 1162 with ops.name_scope(None, op_name, [x, y]) as name: 1163 try: -&gt; 1164 return func(x, y, name=name) 1165 except (TypeError, ValueError) as e: 1166 # Even if dispatching the op failed, the RHS may be a tensor aware /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs) 199 &#34;&#34;&#34;Call target, and fall back on dispatchers if there is a TypeError.&#34;&#34;&#34; 200 try: --&gt; 201 return target(*args, **kwargs) 202 except (TypeError, ValueError): 203 # Note: convert_to_eager_tensor currently raises a ValueError, not a /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in matmul(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name) 3313 else: 3314 return gen_math_ops.mat_mul( -&gt; 3315 a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name) 3316 3317 /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py in mat_mul(a, b, transpose_a, transpose_b, name) 5530 return _result 5531 except _core._NotOkStatusException as e: -&gt; 5532 _ops.raise_from_not_ok_status(e, name) 5533 except _core._FallbackException: 5534 pass /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 6860 message = e.message + (&#34; name: &#34; + name if name is not None else &#34;&#34;) 6861 # pylint: disable=protected-access -&gt; 6862 six.raise_from(core._status_to_exception(e.code, message), None) 6863 # pylint: enable=protected-access 6864 /usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value) InvalidArgumentError: Matrix size-incompatible: In[0]: [3,2], In[1]: [3,2] [Op:MatMul] . Trying to matrix multiply two tensors with the shape (3, 2) errors because the inner dimensions don&#39;t match. . We need to either: . Reshape X to (2, 3) so it&#39;s (2, 3) @ (3, 2). | Reshape Y to (3, 2) so it&#39;s (3, 2) @ (2, 3). | . We can do this with either: . tf.reshape() - allows us to reshape a tensor into a defined shape. | tf.transpose() - switches the dimensions of a given tensor. | . . Let&#39;s try tf.reshape() first. . tf.reshape(Y, shape=(2, 3)) . &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . X @ tf.reshape(Y, shape=(2, 3)) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[ 27, 30, 33], [ 61, 68, 75], [ 95, 106, 117]], dtype=int32)&gt; . It worked, let&#39;s try the same with a reshaped X, except this time we&#39;ll use tf.transpose() and tf.matmul(). . tf.transpose(X) . &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[1, 3, 5], [2, 4, 6]], dtype=int32)&gt; . tf.matmul(tf.transpose(X), Y) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 89, 98], [116, 128]], dtype=int32)&gt; . tf.matmul(a=X, b=Y, transpose_a=True, transpose_b=False) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 89, 98], [116, 128]], dtype=int32)&gt; . Notice the difference in the resulting shapes when tranposing X or reshaping Y. . This is because of the 2nd rule mentioned above: . (3, 2) @ (2, 3) -&gt; (2, 2) done with tf.matmul(tf.transpose(X), Y) | (2, 3) @ (3, 2) -&gt; (3, 3) done with X @ tf.reshape(Y, shape=(2, 3)) | . This kind of data manipulation is a reminder: you&#39;ll spend a lot of your time in machine learning and working with neural networks reshaping data (in the form of tensors) to prepare it to be used with various operations (such as feeding it to a model). . The dot product . Multiplying matrices by eachother is also referred to as the dot product. . You can perform the tf.matmul() operation using tf.tensordot(). . tf.tensordot(tf.transpose(X), Y, axes=1) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 89, 98], [116, 128]], dtype=int32)&gt; . You might notice that although using both reshape and tranpose work, you get different results when using each. . Let&#39;s see an example, first with tf.transpose() then with tf.reshape(). . tf.matmul(X, tf.transpose(Y)) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[ 23, 29, 35], [ 53, 67, 81], [ 83, 105, 127]], dtype=int32)&gt; . tf.matmul(X, tf.reshape(Y, (2, 3))) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[ 27, 30, 33], [ 61, 68, 75], [ 95, 106, 117]], dtype=int32)&gt; . Hmm... they result in different values. . Which is strange because when dealing with Y (a (3x2) matrix), reshaping to (2, 3) and tranposing it result in the same shape. . Y.shape, tf.reshape(Y, (2, 3)).shape, tf.transpose(Y).shape . (TensorShape([3, 2]), TensorShape([2, 3]), TensorShape([2, 3])) . But calling tf.reshape() and tf.transpose() on Y don&#39;t necessarily result in the same values. . print(&quot;Normal Y:&quot;) print(Y, &quot; n&quot;) # &quot; n&quot; for newline print(&quot;Y reshaped to (2, 3):&quot;) print(tf.reshape(Y, (2, 3)), &quot; n&quot;) print(&quot;Y transposed:&quot;) print(tf.transpose(Y)) . Normal Y: tf.Tensor( [[ 7 8] [ 9 10] [11 12]], shape=(3, 2), dtype=int32) Y reshaped to (2, 3): tf.Tensor( [[ 7 8 9] [10 11 12]], shape=(2, 3), dtype=int32) Y transposed: tf.Tensor( [[ 7 9 11] [ 8 10 12]], shape=(2, 3), dtype=int32) . As you can see, the outputs of tf.reshape() and tf.transpose() when called on Y, even though they have the same shape, are different. . This can be explained by the default behaviour of each method: . tf.reshape() - change the shape of the given tensor (first) and then insert values in order they appear (in our case, 7, 8, 9, 10, 11, 12). | tf.transpose() - swap the order of the axes, by default the last axis becomes the first, however the order can be changed using the perm parameter. | . So which should you use? . Again, most of the time these operations (when they need to be run, such as during the training a neural network, will be implemented for you). . But generally, whenever performing a matrix multiplication and the shapes of two matrices don&#39;t line up, you will transpose (not reshape) one of them in order to line them up. . Matrix multiplication tidbits . If we transposed Y, it would be represented as $ mathbf{Y}^ mathsf{T}$ (note the capital T for tranpose). | Get an illustrative view of matrix multiplication by Math is Fun. | Try a hands-on demo of matrix multiplcation: http://matrixmultiplication.xyz/ (shown below). | . . Changing the datatype of a tensor . Sometimes you&#39;ll want to alter the default datatype of your tensor. . This is common when you want to compute using less precision (e.g. 16-bit floating point numbers vs. 32-bit floating point numbers). . Computing with less precision is useful on devices with less computing capacity such as mobile devices (because the less bits, the less space the computations require). . You can change the datatype of a tensor using tf.cast(). . B = tf.constant([1.7, 7.4]) # Create a new tensor with default datatype (int32) C = tf.constant([1, 7]) B, C . (&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.7, 7.4], dtype=float32)&gt;, &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 7], dtype=int32)&gt;) . B = tf.cast(B, dtype=tf.float16) B . &lt;tf.Tensor: shape=(2,), dtype=float16, numpy=array([1.7, 7.4], dtype=float16)&gt; . C = tf.cast(C, dtype=tf.float32) C . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 7.], dtype=float32)&gt; . Getting the absolute value . Sometimes you&#39;ll want the absolute values (all values are positive) of elements in your tensors. . To do so, you can use tf.abs(). . D = tf.constant([-7, -10]) D . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([ -7, -10], dtype=int32)&gt; . tf.abs(D) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 7, 10], dtype=int32)&gt; . Finding the min, max, mean, sum (aggregation) . You can quickly aggregate (perform a calculation on a whole tensor) tensors to find things like the minimum value, maximum value, mean and sum of all the elements. . To do so, aggregation methods typically have the syntax reduce()_[action], such as: . tf.reduce_min() - find the minimum value in a tensor. | tf.reduce_max() - find the maximum value in a tensor (helpful for when you want to find the highest prediction probability). | tf.reduce_mean() - find the mean of all elements in a tensor. | tf.reduce_sum() - find the sum of all elements in a tensor. | Note: typically, each of these is under the math module, e.g. tf.math.reduce_min() but you can use the alias tf.reduce_min(). | . Let&#39;s see them in action. . E = tf.constant(np.random.randint(low=0, high=100, size=50)) E . &lt;tf.Tensor: shape=(50,), dtype=int64, numpy= array([79, 33, 64, 90, 16, 3, 70, 51, 27, 54, 71, 65, 4, 8, 78, 84, 11, 23, 15, 2, 55, 74, 26, 58, 9, 74, 33, 49, 48, 4, 30, 41, 69, 17, 39, 37, 3, 63, 13, 37, 72, 80, 93, 29, 61, 82, 85, 40, 79, 14])&gt; . tf.reduce_min(E) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=2&gt; . tf.reduce_max(E) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=93&gt; . tf.reduce_mean(E) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=45&gt; . tf.reduce_sum(E) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=2262&gt; . You can also find the standard deviation (tf.reduce_std()) and variance (tf.reduce_variance()) of elements in a tensor using similar methods. . Finding the positional maximum and minimum . How about finding the position a tensor where the maximum value occurs? . This is helpful when you want to line up your labels (say [&#39;Green&#39;, &#39;Blue&#39;, &#39;Red&#39;]) with your prediction probabilities tensor (e.g. [0.98, 0.01, 0.01]). . In this case, the predicted label (the one with the highest prediction probability) would be &#39;Green&#39;. . You can do the same for the minimum (if required) with the following: . tf.argmax() - find the position of the maximum element in a given tensor. | tf.argmin() - find the position of the minimum element in a given tensor. | . F = tf.constant(np.random.random(50)) F . &lt;tf.Tensor: shape=(50,), dtype=float64, numpy= array([0.55725069, 0.66589953, 0.51717871, 0.23663905, 0.03861408, 0.25753377, 0.58184723, 0.05496164, 0.97498269, 0.52503693, 0.68152575, 0.8151567 , 0.25854921, 0.15479351, 0.43657365, 0.34710353, 0.18739747, 0.7859743 , 0.11497802, 0.68242342, 0.74991327, 0.16043629, 0.5636553 , 0.4110329 , 0.62245869, 0.58129871, 0.44792704, 0.91472587, 0.76207828, 0.36502321, 0.96601855, 0.03094982, 0.53464709, 0.73241468, 0.0856144 , 0.22923332, 0.66802859, 0.68604535, 0.53128266, 0.48685057, 0.53320669, 0.38020824, 0.81706951, 0.33855374, 0.94007405, 0.05419892, 0.10267899, 0.6020753 , 0.78038818, 0.87655176])&gt; . tf.argmax(F) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=8&gt; . tf.argmin(F) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=31&gt; . print(f&quot;The maximum value of F is at position: {tf.argmax(F).numpy()}&quot;) print(f&quot;The maximum value of F is: {tf.reduce_max(F).numpy()}&quot;) print(f&quot;Using tf.argmax() to index F, the maximum value of F is: {F[tf.argmax(F)].numpy()}&quot;) print(f&quot;Are the two max values the same (they should be)? {F[tf.argmax(F)].numpy() == tf.reduce_max(F).numpy()}&quot;) . The maximum value of F is at position: 8 The maximum value of F is: 0.9749826885110175 Using tf.argmax() to index F, the maximum value of F is: 0.9749826885110175 Are the two max values the same (they should be)? True . Squeezing a tensor (removing all single dimensions) . If you need to remove single-dimensions from a tensor (dimensions with size 1), you can use tf.squeeze(). . tf.squeeze() - remove all dimensions of 1 from a tensor. | . G = tf.constant(np.random.randint(0, 100, 50), shape=(1, 1, 1, 1, 50)) G.shape, G.ndim . (TensorShape([1, 1, 1, 1, 50]), 5) . G_squeezed = tf.squeeze(G) G_squeezed.shape, G_squeezed.ndim . (TensorShape([50]), 1) . One-hot encoding . If you have a tensor of indicies and would like to one-hot encode it, you can use tf.one_hot(). . You should also specify the depth parameter (the level which you want to one-hot encode to). . some_list = [0, 1, 2, 3] # One hot encode them tf.one_hot(some_list, depth=4) . &lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy= array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]], dtype=float32)&gt; . You can also specify values for on_value and off_value instead of the default 0 and 1. . tf.one_hot(some_list, depth=4, on_value=&quot;We&#39;re live!&quot;, off_value=&quot;Offline&quot;) . &lt;tf.Tensor: shape=(4, 4), dtype=string, numpy= array([[b&#34;We&#39;re live!&#34;, b&#39;Offline&#39;, b&#39;Offline&#39;, b&#39;Offline&#39;], [b&#39;Offline&#39;, b&#34;We&#39;re live!&#34;, b&#39;Offline&#39;, b&#39;Offline&#39;], [b&#39;Offline&#39;, b&#39;Offline&#39;, b&#34;We&#39;re live!&#34;, b&#39;Offline&#39;], [b&#39;Offline&#39;, b&#39;Offline&#39;, b&#39;Offline&#39;, b&#34;We&#39;re live!&#34;]], dtype=object)&gt; . Squaring, log, square root . Many other common mathematical operations you&#39;d like to perform at some stage, probably exist. . Let&#39;s take a look at: . tf.square() - get the square of every value in a tensor. | tf.sqrt() - get the squareroot of every value in a tensor (note: the elements need to be floats or this will error). | tf.math.log() - get the natural log of every value in a tensor (elements need to floats). | . H = tf.constant(np.arange(1, 10)) H . &lt;tf.Tensor: shape=(9,), dtype=int64, numpy=array([1, 2, 3, 4, 5, 6, 7, 8, 9])&gt; . tf.square(H) . &lt;tf.Tensor: shape=(9,), dtype=int64, numpy=array([ 1, 4, 9, 16, 25, 36, 49, 64, 81])&gt; . tf.sqrt(H) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-74-d7db039da8bb&gt; in &lt;module&gt;() 1 # Find the squareroot (will error), needs to be non-integer -&gt; 2 tf.sqrt(H) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs) 199 &#34;&#34;&#34;Call target, and fall back on dispatchers if there is a TypeError.&#34;&#34;&#34; 200 try: --&gt; 201 return target(*args, **kwargs) 202 except (TypeError, ValueError): 203 # Note: convert_to_eager_tensor currently raises a ValueError, not a /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in sqrt(x, name) 4901 A `tf.Tensor` of same size, type and sparsity as `x`. 4902 &#34;&#34;&#34; -&gt; 4903 return gen_math_ops.sqrt(x, name) 4904 4905 /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py in sqrt(x, name) 10035 return _result 10036 except _core._NotOkStatusException as e: &gt; 10037 _ops.raise_from_not_ok_status(e, name) 10038 except _core._FallbackException: 10039 pass /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 6860 message = e.message + (&#34; name: &#34; + name if name is not None else &#34;&#34;) 6861 # pylint: disable=protected-access -&gt; 6862 six.raise_from(core._status_to_exception(e.code, message), None) 6863 # pylint: enable=protected-access 6864 /usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value) InvalidArgumentError: Value for attr &#39;T&#39; of int64 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128 ; NodeDef: {{node Sqrt}}; Op&lt;name=Sqrt; signature=x:T -&gt; y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:Sqrt] . H = tf.cast(H, dtype=tf.float32) H . &lt;tf.Tensor: shape=(9,), dtype=float32, numpy=array([1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)&gt; . tf.sqrt(H) . &lt;tf.Tensor: shape=(9,), dtype=float32, numpy= array([1. , 1.4142135, 1.7320508, 2. , 2.236068 , 2.4494898, 2.6457512, 2.828427 , 3. ], dtype=float32)&gt; . tf.math.log(H) . &lt;tf.Tensor: shape=(9,), dtype=float32, numpy= array([0. , 0.6931472, 1.0986123, 1.3862944, 1.609438 , 1.7917595, 1.9459102, 2.0794415, 2.1972246], dtype=float32)&gt; . Manipulating tf.Variable tensors . Tensors created with tf.Variable() can be changed in place using methods such as: . .assign() - assign a different value to a particular index of a variable tensor. | .add_assign() - add to an existing value and reassign it at a particular index of a variable tensor. | . I = tf.Variable(np.arange(0, 5)) I . &lt;tf.Variable &#39;Variable:0&#39; shape=(5,) dtype=int64, numpy=array([0, 1, 2, 3, 4])&gt; . I.assign([0, 1, 2, 3, 50]) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(5,) dtype=int64, numpy=array([ 0, 1, 2, 3, 50])&gt; . I . &lt;tf.Variable &#39;Variable:0&#39; shape=(5,) dtype=int64, numpy=array([ 0, 1, 2, 3, 50])&gt; . I.assign_add([10, 10, 10, 10, 10]) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=(5,) dtype=int64, numpy=array([10, 11, 12, 13, 60])&gt; . I . &lt;tf.Variable &#39;Variable:0&#39; shape=(5,) dtype=int64, numpy=array([10, 11, 12, 13, 60])&gt; . Tensors and NumPy . We&#39;ve seen some examples of tensors interact with NumPy arrays, such as, using NumPy arrays to create tensors. . Tensors can also be converted to NumPy arrays using: . np.array() - pass a tensor to convert to an ndarray (NumPy&#39;s main datatype). | tensor.numpy() - call on a tensor to convert to an ndarray. | . Doing this is helpful as it makes tensors iterable as well as allows us to use any of NumPy&#39;s methods on them. . J = tf.constant(np.array([3., 7., 10.])) J . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 3., 7., 10.])&gt; . np.array(J), type(np.array(J)) . (array([ 3., 7., 10.]), numpy.ndarray) . J.numpy(), type(J.numpy()) . (array([ 3., 7., 10.]), numpy.ndarray) . By default tensors have dtype=float32, where as NumPy arrays have dtype=float64. . This is because neural networks (which are usually built with TensorFlow) can generally work very well with less precision (32-bit rather than 64-bit). . numpy_J = tf.constant(np.array([3., 7., 10.])) # will be float64 (due to NumPy) tensor_J = tf.constant([3., 7., 10.]) # will be float32 (due to being TensorFlow default) numpy_J.dtype, tensor_J.dtype . (tf.float64, tf.float32) . Using @tf.function . In your TensorFlow adventures, you might come across Python functions which have the decorator @tf.function. . If you aren&#39;t sure what Python decorators do, read RealPython&#39;s guide on them. . But in short, decorators modify a function in one way or another. . In the @tf.function decorator case, it turns a Python function into a callable TensorFlow graph. Which is a fancy way of saying, if you&#39;ve written your own Python function, and you decorate it with @tf.function, when you export your code (to potentially run on another device), TensorFlow will attempt to convert it into a fast(er) version of itself (by making it part of a computation graph). . For more on this, read the Better performnace with tf.function guide. . def function(x, y): return x ** 2 + y x = tf.constant(np.arange(0, 10)) y = tf.constant(np.arange(10, 20)) function(x, y) . &lt;tf.Tensor: shape=(10,), dtype=int64, numpy=array([ 10, 12, 16, 22, 30, 40, 52, 66, 82, 100])&gt; . @tf.function def tf_function(x, y): return x ** 2 + y tf_function(x, y) . &lt;tf.Tensor: shape=(10,), dtype=int64, numpy=array([ 10, 12, 16, 22, 30, 40, 52, 66, 82, 100])&gt; . If you noticed no difference between the above two functions (the decorated one and the non-decorated one) you&#39;d be right. . Much of the difference happens behind the scenes. One of the main ones being potential code speed-ups where possible. . Finding access to GPUs . We&#39;ve mentioned GPUs plenty of times throughout this notebook. . So how do you check if you&#39;ve got one available? . You can check if you&#39;ve got access to a GPU using tf.config.list_physical_devices(). . print(tf.config.list_physical_devices(&#39;GPU&#39;)) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . If the above outputs an empty array (or nothing), it means you don&#39;t have access to a GPU (or at least TensorFlow can&#39;t find it). . If you&#39;re running in Google Colab, you can access a GPU by going to Runtime -&gt; Change Runtime Type -&gt; Select GPU (note: after doing this your notebook will restart and any variables you&#39;ve saved will be lost). . Once you&#39;ve changed your runtime type, run the cell below. . import tensorflow as tf print(tf.config.list_physical_devices(&#39;GPU&#39;)) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . If you&#39;ve got access to a GPU, the cell above should output something like: . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . You can also find information about your GPU using !nvidia-smi. . !nvidia-smi . Mon Mar 15 22:33:50 2021 +--+ | NVIDIA-SMI 460.56 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:00:04.0 Off | 0 | | N/A 52C P0 29W / 70W | 224MiB / 15109MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +--+ . 🔑 Note: If you have access to a GPU, TensorFlow will automatically use it whenever possible. . &#128736; Exercises . Create a vector, scalar, matrix and tensor with values of your choosing using tf.constant(). | Find the shape, rank and size of the tensors you created in 1. | Create two tensors containing random values between 0 and 1 with shape [5, 300]. | Multiply the two tensors you created in 3 using matrix multiplication. | Multiply the two tensors you created in 3 using dot product. | Create a tensor with random values between 0 and 1 with shape [224, 224, 3]. | Find the min and max values of the tensor you created in 6. | Created a tensor with random values of shape [1, 224, 224, 3] then squeeze it to change the shape to [224, 224, 3]. | Create a tensor with shape [10] using your own choice of values, then find the index which has the maximum value. | One-hot encode the tensor you created in 9. | &#128214; Extra-curriculum . Read through the list of TensorFlow Python APIs, pick one we haven&#39;t gone through in this notebook, reverse engineer it (write out the documentation code for yourself) and figure out what it does. | Try to create a series of tensor functions to calculate your most recent grocery bill (it&#39;s okay if you don&#39;t use the names of the items, just the price in numerical form). How would you calculate your grocery bill for the month and for the year using tensors? | . | Go through the TensorFlow 2.x quick start for beginners tutorial (be sure to type out all of the code yourself, even if you don&#39;t understand it). Are there any functions we used in here that match what&#39;s used in there? Which are the same? Which haven&#39;t you seen before? | . | Watch the video &quot;What&#39;s a tensor?&quot; - a great visual introduction to many of the concepts we&#39;ve covered in this notebook. | .",
            "url": "https://adam-al-rahman.github.io/LabGarden/jupyter/2021/10/22/00_tensorflow_fundamentals.html",
            "relUrl": "/jupyter/2021/10/22/00_tensorflow_fundamentals.html",
            "date": " • Oct 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://adam-al-rahman.github.io/LabGarden/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://adam-al-rahman.github.io/LabGarden/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://adam-al-rahman.github.io/LabGarden/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://adam-al-rahman.github.io/LabGarden/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}